meta_data:
  script_path: run_scripts/downstream_task.py
  exp_name: offline_policy_improvement
  description: Online sac,Hopper, finetune policy
  num_workers: 12 # 64
  num_gpu_per_worker: 1 # 1 
  num_cpu_per_worker: 4 # 2
  mem_per_worker: 10gb
  partitions: cpu
  node_exclusions: gpu048,gpu024,gpu025,gpu012,gpu027
# -----------------------------------------------------------------------------
variables:
  seed: [9999]
  policy_dir: [100] #100,300,500,800,1200,1450, 1850
  dynamic_dir: ['sn_300_best_r'] # 'sn_1000_best_r','sn_1000_new_r','sn_1000_best_ur','bc_1000_best_r',
  #'bc_1000_best_r','bc_1000_new_r','bc_1000_best_ur','bc_1000_new_ur','bc_10000_best_r','bc_10000_new_r','bc_10000_best_ur','bc_10000_new_ur'
  task_type: [1]

  sac_params:
    policy_lr: [0.0003] #[0.001,0.0001,0.0005,0.00005] [0.0003]
    qf_lr: [0.0003] #[0.001]
    reward_scale: [30.0] # hopper 30
  
  learned_env_params:
    rwd_type: [1]
    penalty_coef: [0]
    deterministic: [true] #true,false

  alg_params:
    max_rollout_length: [1] #10 for hopper, 1 for halfcheetah

# -----------------------------------------------------------------------------
constants:
  test: false # true
  Env_name: 'Hopper'
  # dynamic_dir: 'sn1'

  policy_net_size: 256
  policy_num_hidden_layers: 2    

  sac_params:
    #reward_scale: 5.0
    discount: 0.99
    soft_target_tau: 0.005
    beta_1: 0.25

  learned_env_params:
    use_obs: true
    use_delta: false
    use_gail: false
    # hopper no terminal
    env_observation_mean: [6.84657484, 1.02411338, 0.0831695, -0.55164485, -0.74738959, -0.09371286, 1.62237406, -0.04963518, 0.02073197, -0.0979013, -0.11239976, -0.02732412]
    env_observation_std: [4.82675823, 0.47293847, 0.50263049, 0.70430399, 0.51275484, 0.65485818, 1.11244377, 1.27406441, 1.2982586, 1.76072197, 2.22903456, 5.85081859]
    # env_action_mean: [0.0484465, 0.04688279, -0.26996115]
    # env_action_std: [0.48600423, 0.44863698, 0.66671354] 
  
  alg_params:
    #rollout_length: 10 #10 for hopper, 1 for halfcheetah
    if_set_state: true

    batch_size: 256
    if_save_policy: false
    policy_save_freq: 50

    num_epochs: 1500
    num_steps_per_epoch: 1
    num_steps_between_train_calls: 1
    num_update_loops_per_train_call: 1
    max_path_length: 1000
    min_steps_before_training: 1000

    eval_deterministic: true
    num_steps_per_eval: 1000
    
    replay_buffer_size: 1000000
    no_terminal: false
    wrap_absorbing: false

    save_best: true
    freq_saving: 10
    save_replay_buffer: false
    save_environment: false
    save_algorithm: false
  
  env_specs:
    env_name: 'hopper'
    env_kwargs: {}
    eval_env_seed: 78236
    training_env_seed: 24495